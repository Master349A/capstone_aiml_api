# -*- coding: utf-8 -*-
"""deep-learning-classification-chest-x-ray-images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15b2WXsMvy0W2QX2uj0zfQu7qHzC0IqcI
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/content/drive/MyDrive/data/chest_xray'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

from google.colab import drive
drive.mount('/content/drive')

"""<h2>Step 1: Importing Libraries</h2>
First we import all the libraries that we will need for our program. This includes: 
<ol>
<li> tensorflow - This is the main backbone of our ML model
<li> keras - It is an high level api of tensorflow that is used to create the NN model
<li> sklearn - This lib provides various tools to evaluate the accuracy of the model we train. 
</ol>
"""

import tensorflow as tf
import seaborn as sns
from matplotlib import pyplot as plt
from keras.preprocessing.image import ImageDataGenerator
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

## import keras

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Conv2D
from keras.layers import MaxPool2D
from keras.layers import Flatten 
from keras.layers import Dropout
from keras.layers import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.utils.vis_utils import plot_model

## import sklearn 

from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

print(tf.__version__)
print(keras.__version__)
print(pd.__version__)
!python --version

"""**Training Data** <br>
Here we generate the training data that will be used to train our NN model. We import the dataset and perform augmentation by shearing, rescaling and flipping. This will increase the training space and make it more diverse leading to better accuracy. 
"""

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)
train = train_datagen.flow_from_directory("/content/drive/MyDrive/data/chest_xray/train",
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

"""We do the same for validation data. Validation data is used during training to check the accuracy of model on data outside the training set. This helps the model to avoid overfitting or underfitting on the training dataset."""

val_datagen = ImageDataGenerator(rescale = 1./255)
val = val_datagen.flow_from_directory("/content/drive/MyDrive/data/chest_xray/val",
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

"""**Testing Data**"""

test_datagen = ImageDataGenerator(rescale = 1./255)
test = test_datagen.flow_from_directory("/content/drive/MyDrive/data/chest_xray/test",
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')

"""<h2>ML Model</h2>
Next we define the NN model and train it on training dataset.
<br> 
The model consists of 4 sets of convolution layes with ReLu activation function and MaxPooling layer. 
The last convolutional layer is followed by a dense layer with ReLu activation function.
For the last layer, we have one neuron with sigmoid activation funtion. We are using adam optimizer for optimization and binary cross entropy as loss function.


"""

model = Sequential()
model.add(Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))
model.add(MaxPool2D(pool_size=2, strides=2))
model.add(Conv2D(filters=32, kernel_size=3, activation='relu'))
model.add(MaxPool2D(pool_size=2, strides=2))
model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPool2D(pool_size=2, strides=2))
model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))
model.add(MaxPool2D(pool_size=2, strides=2))
model.add(Flatten())
model.add(Dense(units=128, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
history = model.fit(train,validation_data = val, epochs = 20)

PATH = '/content/drive/MyDrive/data/api1'
model.save(PATH)

## plot loss for train and test

loss_train = history.history['loss']
loss_val = history.history['val_loss']
plt.figure(figsize=(6,4), dpi=100)
plt.plot(loss_train, 'g', label = 'Training loss')
plt.plot(loss_val, 'y', label = 'Validation loss')
plt.title("Training and Testing Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

## plot loss for train and test

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
plt.figure(figsize=(6,4), dpi=100)
plt.plot( loss_train, 'g', label = 'Training accuracy')
plt.plot( loss_val, 'y', label = 'Validation accuracy')
plt.title("Training and Testing accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

results = model.evaluate(test)
print("test loss, test acc:", results)

PATH = '/content/drive/MyDrive/data/api1'
model.save(PATH)

PATH = '/content/drive/MyDrive/data/api1'
cnn = keras.models.load_model(PATH)
print(cnn.summary())

results = cnn.evaluate(test)
print("test loss, test acc:", results)



